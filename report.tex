\documentclass[pageno]{jpaper}

\newcommand{\IWreport}{2014}

\usepackage[normalem]{ulem}
\usepackage{authblk}
\usepackage{wrapfig}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{listings}
\usepackage{appendix}
% \usepackage{subfigure}
% \usepackage{subcaption}
\usepackage{subfig}
\usepackage{placeins}
\usepackage{caption}
% \usepackage[demo]{graphicx}
\makeatletter\def\CT{\def\@captype{figure}}\makeatother


\newcommand{\cwd}{\tt{current\_working\_directory}}

\begin{document}

\title{Exposing a Local Filesystem-Like Interface \\ for Remote Dropbox File Operations}
\author{Dharit Tantiviramanond '15\\ Princeton University \\ dtantivi@princeton.edu}
\affil{Advisor: Christopher Moretti}

\date{}
\maketitle

\doublespace

\thispagestyle{empty}

\begin{abstract}
Remote file storage services are recognized as one of the most cutting-edge technologies of modern day, with Dropbox quickly becoming one of the most well-known and respected tech companies since its launch in 2008. However, many other file storage services exist, and no one vendor has been able to secure a monopoly on this new business. In this paper, I present a first attempt at providing an encapsulation of the Dropbox, Google Drive, and Box APIs under a standard filesystem interface. I then go on to discuss the differences between three upload heuristics that can be implemented, each better suited to a different type of user behavior. I then provide some empirical data related the efficiency for each strategy, and conclude with a discussion of what implications the data holds in store for each design.
\end{abstract}

\section{Introduction}
\label{sec:intro}
The file hosting service market has experienced a boom since the emergence of companies like Dropbox, Box, Copy, Up There, and services like Google Drive, iCloud, and Amazon Cloud Drive. As most of these vendors implement their own Thus far, there has been no successful effort to standardize interaction with each of these vendors, and . This is an obstacle for software developers who want to build applications that work across multiple file storage vendors in order to satisfy the needs of the customer no matter what vendor he or she chooses to use. This project is a first attempt towards solving this problem by implementing a standard and familiar wrapper API around vendor-specific APIS. I started with Dropbox. By making the wrapper API appear as similar to a local filesystem API as possible, programmers will not have to worry about vendor-specific function names and variables, and will thus be able to easily write code that works for multiple services.

The inspiration for this project comes from Parrot, which is project that seeks to act as an ``interposition agent'' between various distributed I/O services and distributed computing services. In brief, Parrot provides a common I/O interface across multiple filesystems, including FTP, HTTP, Chirp, and others. The power of Parrot can be illustrated as thus: instead of having to remember the various arguments, behavior, and functionality of many different commands for many different remote filesystem protocols like $\tt{ftp \ get}$ and HTTP GET requests, users need only $\tt{cd}$ into an imaginary directory corresponding to the desired protocol (eg. $\tt{/ftp/}$ for FTP) and perform the usual filesystem operations like $\tt{read()}$ and $\tt{write()}$ on files located on a remote FTP server from there. To the user, it looks like he is simply operating on files located in a local directory, while Parrot performs all the necessary FTP commands using a strategy known as the Debugger Trap \cite{thain03}.

The programs implemented as part of this project are three 330-line Python modules that seek to mimic similar functionality as Parrot for various file storage services, namely Dropbox, Box, and Google Drive. The idea is to provide interposition layers between the vendor APIs and any applications that would like to connect to them. By providing a familiar API, developers will be able to perform operations on remote files hosted with these services without having to learn the APIs at all.

The Dropbox module is fully functional (it exposes functions that correspond to every function expected to be on a basic filesystem API), while the other two modules are prototypes with limited functionality. The main purpose of these two modules is to test the efficiency and correctness of the implementation across vendors, and to provide data relating to the efficiency of the three different upload heuristics for comparison so that more meaningful vendor-agnostic conclusions can be drawn about the data. The two modules' functionality is limited to uploading files that already exist on the local filesystem, but this was enough to generate the data necessary for this research. For the most part, reconfiguring the baseline code for a new vendor was not very difficult, as most of the functionality that each service was similar to a certain extent. All three vendors initialized with an authorization flow that required the same type of automatic browser activity using the $\tt{splinter}$ package, then created a $\tt{client}$ object from which API calls could be made. All had upload and download functions, and the Dropbox and Box APIs operated in almost the exact same way. The Google Drive API was slightly different, with a very object-oriented API and many parameters. This was the main difficulty that obstructed the full-fledged development of the Google Drive modules--the convoluted API was not conducive towards how I wanted the module to function. For the Box module, the development of the authorization flow ended up taking up the most time. Box has created an app that has the capacity to generate a large number of dummy authentication tokens to be used for testing, and figuring out how this slightly different model of operation worked was a challenge.

To perform operations on files hosted on remote servers, the module must connect to the remote file storage service and transfer files between the local machine and the remote server. This obviously includes uploading modified files back to the servers at some point during the user's session. During the development of the project, I realized that there were many different approaches that could be taken with the uploading scheme--the module could re-upload files to the remote servers after a write operation, after a close operation, or at the end of the user's session. Each of these different schemes provides different levels of efficiency depending on the user's specific behavior during a session, and each has different trade-offs between consistency and efficiency. By consistency, here I mean the consistency between the file's contents on the local filesystem as opposed to its contents on the remote filesystem. Therefore, in order to determine the optimal scheme across all scenarios I decided to test these different schemes against each other across many different examples of possible user sessions. Analysis of the data I gathered is presented in Section~\ref{sec:analysis}. 

\section{Functionality}
\label{sec:functionality}
In this section I will describe the implementations of various mechanisms that allow the module to expose an ordinary filesystem API on the surface, but perform file hosting service API calls under the hood. For this section, I will use Dropbox as the primary example vendor. In effect, the module allows the developer access to all Dropbox operations via a familiar filesystem and I/O API that looks and feels like an ordinary filesystem API. The module's object-oriented nature makes it intuitive and easy to use, and its modular design is conducive towards future development and extension.

The module exposes a class called $\tt{pfs\_service\_dropbox}$ (inheriting the class naming scheme from the original Parrot code) which contains instance methods that function as filesystem operations. It includes the following functions: $\tt{open()}$, $\tt{close()}$, $\tt{read()}$, $\tt{write()}$, $\tt{seek()}$, $\tt{remove()}$, $\tt{chdir()}$, $\tt{mkdir()}$, $\tt{rmdir()}$, $\tt{getcwd()}$, and an $\tt{exit()}$ function to be called at the end of a user's session. These functions' names and functionality are meant to mirror those found the $\tt{os}$ module in the Python standard library \cite{os} as closely as possible. A certain number of private helper functions are also present, as well as a helper class $\tt{pfs\_file\_dropbox}$ to encapsulate all metadata needed when dealing with a particular file. The $\tt{pfs\_file\_dropbox}$ includes members for the file's name, flags, path on the remote server, as well as its file stream object.

\subsection{Directories}
The first thing that the module implements that is not part of the Dropbox API is the concept of a current working directory. The Dropbox API allows for downloading and uploading from arbitrary directories on the remote server by accepting the remote path to the desired file as an input argument to its $\tt{put\_file()}$ upload and $\tt{get\_file()}$ download functions. Therefore, the notion of a current working directory is not necessary to the Dropbox API--as long as developers are able to pass valid and correct paths to $\tt{put\_file()}$, Dropbox will successfully upload the file to the correct location on the remote server. In order to implement a filesystem-like API, however, it is necessary to introduce a mechanism that mimics the functionality of a current working directory. The mechanism is simple: keep a global string $\tt{current\_working\_directory}$ that corresponds to the path of the current working directory. Various directories in the current directory path are delimited by the forward slash character. Changing directories via $\tt{chdir()}$ then becomes a simple matter of concatenating or popping off different /-delimited directory names onto $\cwd$, and $\tt{getcwd()}$ just returns the current value of $\cwd$. Then, to perform operations on a file on a remote Dropbox server, the desired file name is concatenated to the end of $\cwd$ and is passed to the $\tt{put\_file()}$ or $\tt{get\_file()}$ Dropbox API calls, since these functions accept only the full path to the desired file.

Dropbox does support operations for managing remote directories, however. For most of the module's directory-related functions, determining their functionality is a simple matter of mapping a filesystem function to a Dropbox API call. Specifically, $\tt{mkdir()}$ maps to $\tt{file\_create\_folder()}$, while $\tt{rmdir()}$ maps to $\tt{file\_delete()}$, since directories are simply special types of files. Due to this detail, $\tt{remove()}$ also consists of a single call to $\tt{file\_delete()}$.

\subsection{I/O}
Since the Dropbox $\tt{get\_file()}$ function returns a file stream, I had to devise a mechanism by which files could be downloaded, edited, and uploaded back to the server at any point in between edits. I decided to use a simple scheme: download files to a local directory $\tt{dropbox\_dir}$ in the user's root directory, and perform operations on the files from there (the user can easily specify which local directory to use). To keep track of the files, I added a local file descriptor table, which is simply a Python list of $\tt{pfs\_file\_dropbox}$ instances corresponding to each open file. I also had to decide on a naming scheme for the local names of the downloaded files. The obvious first choice would be to not change the file names from their original names on Dropbox at all. However, this leads to naming collisions, since Dropbox does not implement the concept of directories. Two files are allowed to have the same name on the Dropbox server if they are located in different directories, but upon downloading, one will get overwritten since they both have the same name and are both being downloaded to the same local directory. To work around this, I could have named the downloaded files according to their paths and replaced the `/' character with an acceptable substitute, or I could rename the downloaded files with random characters and keep extra metadata in the $\tt{pfs\_file\_dropbox}$ class to keep track of both these random local names and remote paths of each open file. I decided to implement the random naming scheme, as I found it simpler and more elegant.

I also had to decide on when to actually perform the download and upload operations. Downloading is performed with a call to Dropbox's $\tt{get\_file()}$ command that is inside the module's $\tt{open()}$ function. This returns a file stream that is written to a randomly-named local file in $\tt{dropbox\_dir}$. Then, the file is re-opened with the desired flags and the file pointer is encapsulated in a $\tt{pfs\_file\_dropbox}$ instance, which includes metadata about the file's path on the remote server, the file's name, the file's local random name, and the flags with which it was intended to be opened. This $\tt{pfs\_file\_dropbox}$ object is placed in the file descriptor table. Opening a file with the same path as a previously opened file will cause the old file to be closed and replaced with the new file. One issue that I wanted to avoid was cluttering the local $\tt{dropbox\_dir}$ directory, so on a call to $\tt{exit()}$, the program clears out the contents of that directory.

With uploads, however, the situation is far more complicated. As previously mentioned, uploading can be performed after writing to a file, after closing a file, or at the very end of a user session. Depending on which strategy is chosen, then, the call to the Dropbox function $\tt{put\_file()}$ will be inside of $\tt{write()}$, $\tt{close()}$, or $\tt{exit()}$. A more in-depth discussion of the trade-offs between each strategy is in section~\ref{sec:upload}.

Apart from downloading and uploading, the rest of the I/O functionality does not necessitate any interaction with Dropbox. $\tt{read()}$ simply reads from the local file, and $\tt{seek()}$ seeks to the desired location in the local copy of the file as expected.

\subsection{Authentication}
One obstacle that I encountered early on during development was the Dropbox authentication system. Dropbox authentication is performed using the OAuth 2.0 protocol, which was designed to simplify and streamline the authentication process for web apps and mobile apps. Unfortunately, this became a problem for this project as the module is meant to be a Python library, and not a web or mobile app. For web or mobile apps the authentication process looks something like this:
\begin{itemize}
	\item The user is redirected to a Dropbox authentication page where he must enter his credentials.
	\item The user is presented with terms of agreement which he must accept in order to authorize the app to access the user's files hosted on Dropbox.
	\item After accepting, the user is redirected back to the app. An access token is passed to the app, which the app must include as an HTTP header whenever it makes a request to the Dropbox API.
\end{itemize}
For a program running on the command-line, Dropbox recommends presenting the user with a url to the authentication page which the user must copy-paste into a browser. After authentication, the user must copy-paste the access token into the terminal so that the program can use it to make Dropbox API calls. This process is slow and inelegant, and a major hindrance for automated testing. I was able to use the Python Splinter test framework to make a browser automatically visit the authentication page, enter credentials, and click the ``accept'' button in order to automate the authentication process. I made use of the element-searching functionality of Splinter to find the correct text fields to simulate filling in as well as the right buttons elements to simulate clicking. I was, however, unable to avoid opening up a browser all together, although I believe that this could be possible using custom HTTP requests and some clever data scraping from the responses.

\section{Upload Heuristics}
\label{sec:upload}
As previously mentioned, during development I realized that there were many possible ways to implement the uploading mechanism that uploads edited files back to remote file storage servers, and that each would have different benefits and pitfalls depending on the user's usage behavior and scenario. In this section I will examine each strategy in detail, as well as discuss the way in which I measured the three against each other using a testing script.

\subsection{Efficiency vs. Consistency}
In a system where two filesystems are meant to be kept in sync with each other, the two most important qualities to consider are efficiency and consistency. The meaning of efficiency is straightforward: lagging applications and slow response times are exasperating and inhibit productivity, and thus we would like to strive for the most efficient system possible. Consistency refers to minimizing the difference in state between the two filesystems, in this case the local filesystem and the remote file storage server. The concept can be made clear through an example: if a user edits a file that is in his Dropbox folder on his local machine (that is meant to be kept in sync with a copy of the same file on his remote Dropbox account), after editing he should expect to be able to view his changes on his remote account immediately. Of course, this is impossible since uploading the changes to Dropbox requires a certain amount of time.

The trade-off between efficiency and consistency then becomes apparent: an upload strategy that ensures maximum efficiency would perform uploads rarely, and in the background, in order to minimize the overhead associated with every upload operation. And since many edits to files often overwrite or undo previous changes, it is more efficient to only transfer the most recent edits in a single upload, rather than transfer all edits every step of the way only to have them overwritten later. On the other hand, a strategy that ensures maximum consistency would perform uploads as often as possible, as this minimizes the time during which there are conflicts between two copies of the same file on the two different filesystems.

In this project, I implemented three different upload strategies that are numbered as modes 0 through 2 as below. For simplicity, I will refer to the three strategies as these modes throughout the rest of this report. I also offer some remarks on the expected levels of efficiency and consistency for each mode:
\begin{itemize}
	\item Mode 0: upload on write. This strategy minimizes the time spent in an inconsistent state between the local and remote filesystems since the upload is performed immediately after the file is edited. The only time there are any inconsistencies is during the upload operation itself. However, this strategy is expected to become inefficient if many writes are performed in succession.
	\item Mode 1: upload on close. This strategy represents a middle ground between efficiency and consistency. During the time that a file is kept open, after any edits the file will be inconsistent with its copy on the remote server. Once the file is closed, however, consistency will be restored. And since uploads are performed after files are closed, no efficiency is lost by uploading edits that will be overwritten later.
	\item Mode 2: upload on exit. This strategy is the most efficient of the three, but also opens the door for the most inconsistencies. During the entire session, any edits that the user makes to local files will not be seen on the remote server until after the user intentionally ends his session. If the user tries to access the files on the file storage vendor's web access point at any point during the session, problems can occur as none of his changes have been uploaded yet. However, only a single upload operation is needed for every file that the user opens during the session.
\end{itemize}

\subsection{Data Collection}
I first enabled data collection by making the $\tt{pfs\_service\_dropbox}$ class accept an int in the range 0-2 corresponding to the desired mode as an input argument. I implemented the three different schemes by having each function branch off to the correct functionality based on which mode number was passed to it. For example, $\tt{close()}$ would not upload the file on mode 0 or 2, but would upload the file on mode 1. However, on mode 2 it would store the local file name in a list to be uploaded later during $\tt{exit()}$.

I coded a 320-line testing script comprising 8 tests, each modeling a different user session. I tried to capture the possible extremes of the sessions in relation to the upload schemes. For example, I included a test with many small writes, which would cause a consistency-focused strategy to quickly become inefficient. Following is a short description of the 8 tests:
\begin{enumerate}[start=0]
\item Simple test that just opens a file, performs a write, then closes the file and exits.
\item 6 small write operations of size ranging between 12 and 33 characters each in a single session.
\item 20 single-character write operations in a single session.
\item Open a file in r+ mode and perform a read and write, then open the same file in w+ mode and perform a single large write.
\item 10 alternating write and close operations.
\item Three files are opened concurrently and a single large write is performed on each. The files are closed at the same time at the end of the session.
\item Three files are opened concurrently and a single large write is performed on each. The files are written to and immediately closed afterwards one by one.
\item Three files opened concurrently, with a series of read, write, and seek operations performed in a pseudo-random fashion.
\end{enumerate}
I tried to isolate the upload speeds as much as possible. I did not test download speeds when they were not necessary to the test, and I did not include the authentication process in the timings either. Timing started after the files were downloaded (via the $\tt{open()}$ function) and ended after $\tt{exit()}$ was called. I tested across three dimensions: mode, file size, and test case. For smaller sizes, I ran about 50 iterations per test, and for larger sizes I tried to keep the number of iterations above 20. The script generates bar-graph representations of the means and standard deviations of the data automatically using the matplotlib and numpy Python packages.

\section{Analaysis}
\label{sec:analysis}
In order to fully examine the test output, I will examine the tests one by one. I will focus on the dropbox test results, and use the Google Drive and Box results as comparisons in order to make remarks about how reliable the test data is as well as any implications as to the performance of each vendor. Please note that all timings in the figures are in seconds.
\begin{enumerate}[label=Test \arabic*:,start=0]
\item See figures~\ref{dtest0}, \ref{gtest0}, and \ref{btest0}. As this is just a baseline test with one call to $\tt{write()}$, $\tt{close()}$, and $\tt{exit()}$ each, the timings across every vendor and every mode should be comparable. Indeed, the data confirms this notion.

\FloatBarrier
\begin{center}
\CT
\subfloat[\label{dtest0}]{\includegraphics[height=8cm]{dtest0.pdf}}\quad
\subfloat[\label{gtest0}]{\includegraphics[height=8cm]{gtest0.pdf}}\quad
\subfloat[\label{btest0}]{\includegraphics[height=8cm]{btest0.pdf}}
\captionof{figure}{}
\end{center}

\item See figures~\ref{dtest1}, \ref{gtest1}, and \ref{btest1}. The data indicates that mode 1 is the most costly here, as is expected since this is a test with many small write operations before a single $\tt{close()}$ or $\tt{exit()}$ is called. And as expected, modes 1 and 2 use almost exactly the same amount of time. Interestingly, Dropbox's upload speeds far outstrip those of other vendors at less than 0.03 seconds for files up to 100 kb large, with Google drive taking about 0.1 seconds and Box taking more than 0.15 seconds. This implies that Dropbox has managed to decrease its per-upload overhead far more successfully than at least these two competitors. For larger file sizes, the difference is less noticeable. This may be because for larger files, most vendors may break the file down into smaller packets, send each packet individually across the network, and reconstruct the original file on the receiving end. Another interesting point is that for Box and Google Drive, at file sizes of 1kb through 100kb, the upload speeds appear to be somewhat constant. This may be because these vendors upload schemes necessitate a minimum upload time. One such scheme may be to divide every file into equal-sized chunks and upload each chunk individually, regardless of file size.

\begin{center}
\CT
\subfloat[\label{dtest1}]{\includegraphics[height=9cm]{dtest1.pdf}}\quad
\subfloat[\label{gtest1}]{\includegraphics[height=9cm]{gtest1.pdf}}\quad
\subfloat[\label{btest1}]{\includegraphics[height=9cm]{btest1.pdf}}
\captionof{figure}{}
\end{center}

\item See figures~\ref{dtest2}, \ref{gtest2}, and \ref{btest2}. I included this test to observe the growth of the upload times as the number of write operations increased. The results are somewhat surprising. For Box and Google Drive on mode 0, as the number of write operations increased by a factor of about 3.3 one may expect the upload speeds to increase by a similar factor as well, since the size difference of the changes made did not increase by any relatively large factor. However, the upload times increased by a slightly smaller factor than 3.3 in both cases, which suggests that these vendors' uploading schemes may be a little smarter than simply dividing into equal-sized chunks regardless of file size. Perhaps these vendors reduce the number of chunks as the file size increases; this may make sense up to a certain file size upper bound. Also noteworthy is the fact that the upload speeds for the other two modes actually decreased. The only factor that decreased between tests 1 and 2 is the total size of the changes made, and since on modes 1 and 2 only one upload operation was performed, the amount of information needed to update the remote copy of the file would have decreased.

For Dropbox, however, the mean upload times increased to about 0.2 seconds for files of sizes 1-100kb, and the upper bound for the standard deviations increased to about 0.4 seconds for the same file sizes. One possible explanation may be that since Dropbox tracks changes (in a somewhat source-control-like fashion; the $\tt{delta()}$ function of the Dropbox API provides developers with instructions on how update a file to match the file's state on the remote server), every change uploaded to the server may incur the same amount of overhead cost. With the large spread of figure~\ref{dtest2}'s standard deviation, however, it is hard to generalize. Finally, for all file sizes, Dropbox still remains the fastest in terms of upload speeds.

\begin{center}
\CT
\subfloat[\label{dtest2}]{\includegraphics[height=9cm]{dtest2.pdf}}\quad
\subfloat[\label{gtest2}]{\includegraphics[height=9cm]{gtest2.pdf}}\quad
\subfloat[\label{btest2}]{\includegraphics[height=9cm]{btest2.pdf}}
\captionof{figure}{}
\end{center}

\item See figures~\ref{dtest3}, \ref{gtest3}, and \ref{btest3}. As expected, the upload times for every mode remains at a relatively constant level, as the same number of $\tt{write()}$, $\tt{close()}$, and $\tt{exit()}$ operations are performed. Somewhat noteworthy is the fact that 1mb files, mode 2 seems to be consistently very slightly faster than the other two modes across all three vendors. Also, this is the first test that Dropbox fails miserably when compared to the other vendors. For 1mb files, Dropbox takes about 14 seconds, while the upload times for both Box and Google Drive are upper bounded by 2 seconds. From observing the data, Dropbox's upload speeds increase by about a factor of 10, linear to the increase in file size.

\begin{center}
\CT
\subfloat[\label{dtest3}]{\includegraphics[height=9cm]{dtest3.pdf}}\quad
\subfloat[\label{gtest3}]{\includegraphics[height=9cm]{gtest3.pdf}}\quad
\subfloat[\label{btest3}]{\includegraphics[height=9cm]{btest3.pdf}}
\captionof{figure}{}
\end{center}

\item See figures~\ref{dtest4}, \ref{gtest4}, and \ref{btest4}. The first noticeable point in this test is that for Box and Google Drive, the upload speeds behave mostly as expected. Modes 0 and 1 take about a constant amount of time, while mode 2 is faster by a factor of 7-10. This makes sense, as the number of $\tt{write()}$ and $\tt{close()}$ operations in this test are 10 times as much as the number of calls to $\tt{exit()}$.

Also noteworthy is that Dropbox's upload times are again much worse than its competitors. For 1mb files, modes 1 and 2 reach speeds almost as slow as 200 seconds, and mode 1 takes over 150 seconds. The size of the changes made to the files is not out of the ordinary, and neither is the number of changes made. Follow-up tests for Dropbox confirmed the results.

\begin{center}
\CT
\subfloat[\label{dtest4}]{\includegraphics[height=9cm]{dtest4.pdf}}\quad
\subfloat[\label{gtest4}]{\includegraphics[height=9cm]{gtest4.pdf}}\quad
\subfloat[\label{btest4}]{\includegraphics[height=9cm]{btest4.pdf}}
\captionof{figure}{}
\end{center}

\item See figures~\ref{dtest5}, \ref{gtest5}, and \ref{btest5}. Upload speed results behave much as expected. Since the number of calls to each function is the same, the upload speeds are consistent across all three modes. Dropbox takes the least time, Box the most.

\begin{center}
\CT
\subfloat[\label{dtest5}]{\includegraphics[height=9cm]{dtest5.pdf}}\quad
\subfloat[\label{gtest5}]{\includegraphics[height=9cm]{gtest5.pdf}}\quad
\subfloat[\label{btest5}]{\includegraphics[height=9cm]{btest5.pdf}}
\captionof{figure}{}
\end{center}

\item See figures~\ref{dtest6}, \ref{gtest6}, and \ref{btest6}. This test is similar to the previous test, except the ordering of the operations is different. I was interested to see if there would be any differences in the upload speeds as a result of the order of the files being written to on the remote server, but there were none. Somewhat noteworthy, however, is that the uploads took about 1 second longer for both Box and Google Drive at 1mb files.

\begin{center}
\CT
\subfloat[\label{dtest6}]{\includegraphics[height=9cm]{dtest6.pdf}}\quad
\subfloat[\label{gtest6}]{\includegraphics[height=9cm]{gtest6.pdf}}\quad
\subfloat[\label{btest6}]{\includegraphics[height=9cm]{btest6.pdf}}
\captionof{figure}{}
\end{center}

\item See figures~\ref{dtest7}, \ref{gtest7}, and \ref{btest7}. Here Dropbox again takes the longest to upload, with speeds reaching 100 seconds for mode 0 and more than 60 for mode 2. For all three vendors, mode 2 takes the least amount of time which is expected, as the number of calls to $\tt{exit()}$ is lower than the number of calls to $\tt{write()}$ and $\tt{close()}$. Interestingly, mode 0 takes the longest for Box, whereas mode 1 takes the longest for Google Drive. This is the only test where two different modes are noticeably dominant for two different vendors across all file sizes. The implications of this fact are hard to decipher, as the operations don't follow any particular pattern.

\begin{center}
\CT
\subfloat[\label{dtest7}]{\includegraphics[height=9cm]{dtest7.pdf}}\quad
\subfloat[\label{gtest7}]{\includegraphics[height=9cm]{gtest7.pdf}}\quad
\subfloat[\label{btest7}]{\includegraphics[height=9cm]{btest7.pdf}}
\captionof{figure}{}
\end{center}

\end{enumerate}

\section{Conclusion}
It is hard to tell which mode is the best judging only from the data, as all three modes behave as expected. If taken at face value, both modes 0 and 1 are the slowest modes for the tests where the upload speeds aren't equal across all three modes, but mode 2 is never the slowest. This is not unexpected, however. In the earlier discussion about the trade-off between consistency and efficiency, it was made clear that strategies that prefer to only upload once every so often instead of uploading as frequently as possible would do better in terms of overall efficiency, in exchange for a certain risk of inconsistency. Mode 2 most fits this description out of all the modes, and since the tests only measured efficiency, this result is expected. Indeed, because of this mode 2 has one drawback: if the user ever went into his remote Dropbox account and edited the same file he had open during a local session, one or the other of the two changes would get overwritten, depending on the type of transfer the user makese first.

Would tests that measured consistency across the local machine and remote server in terms of some sort of metric revealed any meaningful information? Perhaps, but one must also consider the fact that if a user has a file open during a local session, it is not likely that he will have the need or desire to edit the file on the remote server as well, before ending his local session. Even if the user is unaware of the consequences of such an action, only under certain circumstances would the user really have the need to do this (eg. the user left an open file on a computer and switched to using a different computer to do the same work). Therefore, for most practical purposes, mode 2 is probably the best mode to choose out of the three. It is the most efficient, and the risk associated with inconsistency that comes along with it is not very high.

\section{Discussion}
Deterministic upload strategies can only go so far before hitting a wall. Designing more competitive algorithms may have to involve probabilistic, non-deterministic strategies. These types of strategies may sometimes fail to outperform their deterministic counterparts, but usually perform better overall. Implementing such an algorithm would involve choosing probabilities to associate with each pure strategy, according to which each strategy was chosen for any particular session or action.

One obstacle that was posed to me during a presentation of this project was the issue of multiple users editing a file at the same time. All three vendors offer sharing functionality that allow two or more users to make changes concurrently. At the time I answered that such concerns were beyond the scope of the project, as I was more focused on examining particular upload schemes as pertaining to the communication between a client and server of a remote file storage service. Pondering the question does yield some interesting discussion, however. How would adding the dimension of upload scheme between client and server affect concurrent behavior? Consider the case where two users attempt to edit the same part of the same file in parallel. Mode 0 would be the only somewhat acceptable mode to operate under, and even then, one's activity may only be observable to the other in lagging bursts, which would make such an endeavor difficult. Modes 1 and 2 would most likely result in failure, as one user would be required to close the file he was working on or end his session altogether before his changes would be seen by the other. It may be for this reason that a cross-vendor remote file storage access application has failed to come about.

\section{Further Work}
One natural further step to take with this project would be to build up the Box and Google Drive modules to a functional level, and to build similar modules for other remote file storage vendors. With enough vendors, it may become useful to package all the modules together to form a single library through which developers could write applications to access a user's remote file storage account in a completely vendor-agnostic manner.

Another possible idea would be to examine the mathematical competitiveness of each mode against an optimal algorithm that has knowledge of the entire user's behavior for any given session beforehand. Information about this could shed light on how one would go about designing and implementing a non-deterministic strategy that could incorporate some or all of the three modes presented here. 

\vspace{1cm}
This paper represents my own work in accordance with University regulations.
\vspace{1cm}

\bstctlcite{bstctl:etal, bstctl:nodash, bstctl:simpurl}
\bibliographystyle{IEEEtranS}
\bibliography{references}

% \clearpage
% \appendix
% \appendixpage
% Please note that all timings are in seconds.
% \FloatBarrier

% \begin{figure}
% 	\caption{\label{dtest0}}
%     \begin{center}
% 	    \includegraphics[width=1\textwidth]{dtest0.pdf}
% 	\end{center}
% \end{figure}
% \begin{figure}
% 	\caption{\label{gtest0}}
%     \begin{center}
% 	    \includegraphics[width=1\textwidth]{gtest0.pdf}
% 	\end{center}
% \end{figure}
% \begin{figure}
% 	\caption{\label{btest0}}
%     \begin{center}
% 	    \includegraphics[width=1\textwidth]{btest0.pdf}
% 	\end{center}
% \end{figure}
% \begin{figure}
% 	\caption{\label{dtest1}}
%     \begin{center}
% 	    \includegraphics[width=1\textwidth]{dtest1.pdf}
% 	\end{center}
% \end{figure}
% \begin{figure}
% 	\caption{\label{gtest1}}
%     \begin{center}
% 	    \includegraphics[width=1\textwidth]{gtest1.pdf}
% 	\end{center}
% \end{figure}
% \begin{figure}
% 	\caption{\label{btest1}}
%     \begin{center}
% 	    \includegraphics[width=1\textwidth]{btest1.pdf}
% 	\end{center}
% \end{figure}
% \begin{figure}
% 	\caption{\label{dtest2}}
%     \begin{center}
% 	    \includegraphics[width=1\textwidth]{dtest2.pdf}
% 	\end{center}
% \end{figure}
% \begin{figure}
% 	\caption{\label{gtest2}}
%     \begin{center}
% 	    \includegraphics[width=1\textwidth]{gtest2.pdf}
% 	\end{center}
% \end{figure}
% \begin{figure}
% 	\caption{\label{btest2}}
%     \begin{center}
% 	    \includegraphics[width=1\textwidth]{btest2.pdf}
% 	\end{center}
% \end{figure}
% \clearpage
% \begin{figure}
% 	\caption{\label{dtest3}}
%     \begin{center}
% 	    \includegraphics[width=1\textwidth]{dtest3.pdf}
% 	\end{center}
% \end{figure}
% \begin{figure}
% 	\caption{\label{gtest3}}
%     \begin{center}
% 	    \includegraphics[width=1\textwidth]{gtest3.pdf}
% 	\end{center}
% \end{figure}
% \begin{figure}
% 	\caption{\label{btest3}}
%     \begin{center}
% 	    \includegraphics[width=1\textwidth]{btest3.pdf}
% 	\end{center}
% \end{figure}
% \begin{figure}
% 	\caption{\label{dtest4}}
%     \begin{center}
% 	    \includegraphics[width=1\textwidth]{dtest4.pdf}
% 	\end{center}
% \end{figure}
% \begin{figure}
% 	\caption{\label{gtest4}}
%     \begin{center}
% 	    \includegraphics[width=1\textwidth]{gtest4.pdf}
% 	\end{center}
% \end{figure}
% \begin{figure}
% 	\caption{\label{btest4}}
%     \begin{center}
% 	    \includegraphics[width=1\textwidth]{btest4.pdf}
% 	\end{center}
% \end{figure}
% \begin{figure}
% 	\caption{\label{dtest5}}
%     \begin{center}
% 	    \includegraphics[width=1\textwidth]{dtest5.pdf}
% 	\end{center}
% \end{figure}
% \begin{figure}
% 	\caption{\label{gtest5}}
%     \begin{center}
% 	    \includegraphics[width=1\textwidth]{gtest5.pdf}
% 	\end{center}
% \end{figure}
% \begin{figure}
% 	\caption{\label{btest5}}
%     \begin{center}
% 	    \includegraphics[width=1\textwidth]{btest5.pdf}
% 	\end{center}
% \end{figure}
% \begin{figure}
% 	\caption{\label{dtest6}}
%     \begin{center}
% 	    \includegraphics[width=1\textwidth]{dtest6.pdf}
% 	\end{center}
% \end{figure}
% \begin{figure}
% 	\caption{\label{gtest6}}
%     \begin{center}
% 	    \includegraphics[width=1\textwidth]{gtest6.pdf}
% 	\end{center}
% \end{figure}
% \begin{figure}
% 	\caption{\label{btest6}}
%     \begin{center}
% 	    \includegraphics[width=1\textwidth]{btest6.pdf}
% 	\end{center}
% \end{figure}
% \begin{figure}
% 	\caption{\label{dtest7}}
%     \begin{center}
% 	    \includegraphics[width=1\textwidth]{dtest7.pdf}
% 	\end{center}
% \end{figure}
% \begin{figure}
% 	\caption{\label{gtest7}}
%     \begin{center}
% 	    \includegraphics[width=1\textwidth]{gtest7.pdf}
% 	\end{center}
% \end{figure}
% \begin{figure}
% 	\caption{\label{btest7}}
%     \begin{center}
% 	    \includegraphics[width=1\textwidth]{btest7.pdf}
% 	\end{center}
% \end{figure}

\end{document}
